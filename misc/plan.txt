void backpropagation(Network *network, double *truth, double learning_rate) {
    Layer *output_layer = network->layers;
    Layer *layer_pt = output_layer;
    Neuron *neuron_pt;
    Weight *weight_pt;

    while (layer_pt->next != NULL) layer_pt = layer_pt->next;
    neuron_pt = layer_pt->neurons;

    for (int i = 0; i < layer_pt->size; i++) {
        double output = neuron_pt->activation;
        double target = truth[i];

        // Calculate the delta for the output layer
        double output_error = (output - target) * sigmoid_derivative(output);
        neuron_pt->delta = output_error;

        // Calculate the gradients for the output layer weights (not needed)
        // weight_pt = neuron_pt->weights;
        // while (weight_pt != NULL) {
        //     weight_pt->gradient = output_error * neuron_pt->activation;
        //     weight_pt = weight_pt->next;
        // }

        neuron_pt = neuron_pt->next;
    }

    layer_pt = layer_pt->prev;
    while (layer_pt != NULL) {
        neuron_pt = layer_pt->neurons;

        for (int i = 0; i < layer_pt->size; i++) {
            double weighted_sum = 0;
            Neuron *next_layer_neuron = layer_pt->next->neurons;

            // Calculate the weighted sum of deltas from the next layer
            while (next_layer_neuron != NULL) {
                Weight *next_layer_weight = next_layer_neuron->weights;
                weighted_sum += next_layer_neuron->delta * next_layer_weight->val;
                next_layer_neuron = next_layer_neuron->next;
            }

            double output = neuron_pt->activation;

            // Calculate the delta for the current layer
            double hidden_error = weighted_sum * sigmoid_derivative(output);
            neuron_pt->delta = hidden_error;

            // Calculate the gradients for the current layer weights
            weight_pt = neuron_pt->weights;
            while (weight_pt != NULL) {
                weight_pt->gradient = hidden_error * neuron_pt->activation;
                weight_pt = weight_pt->next;
            }

            neuron_pt = neuron_pt->next;
        }

        layer_pt = layer_pt->prev;
    }

    update_weights(network, learning_rate); // Update the weights after calculating gradients and deltas
}

Backpropagation(Network *network, double *truth, double learning_rate):
    // Forward Propagation
    For each layer L in the network:
        For each neuron N in layer L:
            If L is the input layer:
                Set the input activation of N to the corresponding feature from the sample.
            Else:
                Calculate the weighted sum of activations from the previous layer for N.
                Apply the activation function to get the output activation of N.

    // Backward Propagation
    For each layer L in the network (in reverse order, from the output layer to the second hidden layer):
        For each neuron N in layer L:
            If L is the output layer:
                Calculate the delta for N using the output activation and the corresponding target value from 'truth'.
            Else:
                Calculate the weighted sum of deltas from the next layer for N.
                Calculate the delta for N using the weighted sum and the derivative of the activation function.

    // Update the gradients for the weights and biases
    For each layer L in the network (from the second hidden layer to the output layer):
        For each neuron N in layer L:
            For each weight W in the weights of N:
                Calculate the gradient for W using the delta of N and the activation of the connected neuron from the previous layer.
            Update the bias of N using its delta.

    // Average the gradients over the entire batch (if using mini-batch gradient descent)
    // If using batch gradient descent, this step is not needed.

    // Update the weights and biases
    For each layer L in the network (from the second hidden layer to the output layer):
        For each neuron N in layer L:
            For each weight W in the weights of N:
                Update the weight W using the gradient and the learning_rate.
            Update the bias of N using the delta and the learning_rate.

