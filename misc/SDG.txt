void backward_pass(Network * network, double truth[], int truth_size, double learning_rate, double lambda)
{
    Layer  * main_layer = network -> layers;
    Layer  * layer_pt   = main_layer;
    Layer  * next_layer;
    Neuron * neuron_pt;
    Weight * weight_pt;

    double current_delta;

    while(layer_pt -> next != NULL) layer_pt = layer_pt -> next; // get last layer (output layer)

    // calculate deltas and biases for output layer
    neuron_pt = layer_pt -> neurons;
    for(int i = 0; i < truth_size; i++)
    {
        neuron_pt -> delta = ((neuron_pt -> activation) - truth[i]); // calculate delta
        neuron_pt -> bias -= learning_rate * (neuron_pt -> delta);   // update the bias
        neuron_pt =  neuron_pt -> next;
    }

    next_layer = layer_pt;
    layer_pt = layer_pt -> prev;
    while(layer_pt != NULL)
    {
        // calculate deltas
        neuron_pt = layer_pt -> neurons;
        while(neuron_pt != NULL)
        {
            current_delta = 0;

            // update our weights
            weight_pt = neuron_pt -> weights;
            while(weight_pt != NULL)
            {
                current_delta += (weight_pt -> to_neuron -> delta) * (weight_pt -> val);
                // weight_pt -> gradient = (weight_pt -> to_neuron -> delta) * (neuron_pt -> activation);
                weight_pt -> gradient = ((weight_pt -> to_neuron -> delta) * (neuron_pt -> activation)) + (lambda * sign(weight_pt -> val)); // our gradient with L1 regularization
                weight_pt -> val -= learning_rate * (weight_pt -> gradient);
                weight_pt = weight_pt -> next;
            }

            // update our biases only if its not the input layer
            if(layer_pt -> prev != NULL)
            {
                neuron_pt -> delta = current_delta * relu_derivative(neuron_pt -> activation);
                neuron_pt -> bias -= learning_rate * (neuron_pt -> delta);
            }

            neuron_pt = neuron_pt -> next;
        }
        next_layer = next_layer -> prev;
        layer_pt   = layer_pt -> prev;
    }
    network -> layers = main_layer;
}

void apply_gradient(Network * network, double learning_rate)
{
    Layer  * main_layer = network -> layers;
    Layer  * layer_pt   = main_layer;
    Neuron * neuron_pt;
    Weight * weight_pt;

    network -> layers = main_layer;
}

void epoch(Network * network, double ** data, int num_features, int num_samples)
{
    double sample_value;
    double input[784];
    double average_loss = 0;
    
    double learning_rate = LEARNING_RATE * pow(2.0, (double) (network -> current_epoch) / -100.0);
    double lambda        = REGULAR_TERM  * pow(2.0, (double) (network -> current_epoch) / -50.0);

    int index[num_samples];
    for(int i = 0; i < num_samples; i++)
    {
        index[i] = i;
    }
    fishers_yates_shuffle(index, num_samples);

    for(int i = 0; i < num_samples; i++)
    {
        double truth[] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0};

        sample_value = data[index[i]][0];
        for(int j = 0; j < 10; j++) if(j == (int) sample_value) truth[j] = 1; // create truth array
        for(int j = 1; j < num_features; j++) input[j] = data[index[i]][j];   // create input array

        average_loss += forward_pass(network, truth, 10, input, 784);
        backward_pass(network, truth, 10, learning_rate, lambda);
    }

    average_loss /= (double) num_samples;

    (network -> current_epoch)++;
    printf("Average loss: %lf\n", average_loss);
}